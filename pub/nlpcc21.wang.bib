@InProceedings{10.1007/978-3-030-88480-2_61,
author="Wang, Cunxiang
and Zheng, Boyuan
and Niu, Yuchen
and Zhang, Yue",
editor="Wang, Lu
and Feng, Yansong
and Hong, Yu
and He, Ruifang",
title="Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning",
booktitle="Natural Language Processing and Chinese Computing",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="758--769",
abstract="To quantitatively and intuitively explore the generalization ability of pre-trained language models (PLMs), we have designed several tasks of arithmetic and logical reasoning. We both analyse how well PLMs generalize when the test data is in the same distribution as the train data and when it is different, for the latter analysis, we have also designed a cross-distribution test set other than the in-distribution test set. We conduct experiments on one of the most advanced and publicly released generative PLM - BART. Our research finds that the PLMs can easily generalize when the distribution is the same, however, it is still difficult for them to generalize out of the distribution.",
isbn="978-3-030-88480-2"
}

