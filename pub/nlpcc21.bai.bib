@InProceedings{10.1007/978-3-030-88480-2_9,
author="Bai, Xuefeng
and Li, Yafu
and Zhang, Zhirui
and Xu, Mingzhou
and Chen, Boxing
and Luo, Weihua
and Wong, Derek
and Zhang, Yue",
editor="Wang, Lu
and Feng, Yansong
and Hong, Yu
and He, Ruifang",
title="Sentence-State LSTMs For Sequence-to-Sequence Learning",
booktitle="Natural Language Processing and Chinese Computing",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="104--115",
abstract="Transformer is currently the dominant method for sequence to sequence problems. In contrast, RNNs have become less popular due to the lack of parallelization capabilities and the relatively lower performance. In this paper, we propose to use a parallelizable variant of bi-directional LSTMs (BiLSTMs), namely sentence-state LSTMs (S-LSTM), as an encoder for sequence-to-sequence tasks. The complexity of S-LSTM is only {\$}{\$}{\backslash}mathcal {\{}O{\}}(n){\$}{\$}O(n)as compared to {\$}{\$}{\backslash}mathcal {\{}O{\}}(n^2){\$}{\$}O(n2)of Transformer. On four neural machine translation benchmarks, we empirically find that S-SLTM can achieve significantly better performances than BiLSTM and convolutional neural networks (CNNs). When compared to Transformer, our model gives competitive performance while being 1.6 times faster during inference.",
isbn="978-3-030-88480-2"
}

