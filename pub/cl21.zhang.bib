@article{10.1162/coli_r_00389,
    author = {Zhang, Yue},
    title = "{Deep Learning Approaches to Text
          Production}",
    journal = {Computational Linguistics},
    volume = {46},
    number = {4},
    pages = {899-903},
    year = {2021},
    month = {02},
    abstract = "{Text production (Reiter and Dale 2000; Gatt and Krahmer 2018) is also
            referred to as natural language generation (NLG). It is a subtask of
            natural language processing focusing on the generation of natural language text.
            Although as important as natural language understanding for communication, NLG had
            received relatively less research attention. Recently, the rise of deep learning
            techniques has led to a surge of research interest in text production, both in general
            and for specific applications such as text summarization and dialogue systems. Deep
            learning allows NLG models to be constructed based on neural representations, thereby
            enabling end-to-end NLG systems to replace traditional pipeline approaches, which frees
            us from tedious engineering efforts and improves the output quality. In particular, a
            neural encoder-decoder structure (Cho et al. 2014; Sutskever, Vinyals, and Le 2014) has been widely used as a basic framework, which computes input
            representations using a neural encoder, according to which a text
            sequence is generated token by token using a neural decoder. Very
            recently, pre-training techniques (Broscheit et al. 2010; Radford 2018; Devlin et al. 2019) have further allowed neural models to
            collect knowledge from large raw text data, further improving the quality of both
            encoding and decoding.}",
    issn = {0891-2017},
    doi = {10.1162/coli_r_00389},
    url = {https://doi.org/10.1162/coli\_r\_00389},
    eprint = {https://direct.mit.edu/coli/article-pdf/46/4/899/1888270/coli\_r\_00389.pdf},
}



